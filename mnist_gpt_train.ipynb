{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca14a5e-c286-4d67-87c6-b8203d65e04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aba42d-a867-4d49-8b1c-34f19910c87b",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17a54337-bf6c-4a5b-ad55-ec615d35cf63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f828850ff50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data.iloc[1].values[1:].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54450440-f39c-4600-973d-0de14bbbf4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.stop_token = 256 + 10 # 256 - 265 correspond to labels, which we can condition on\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label_token = self.data.iloc[idx].values[0] + 256\n",
    "        data = torch.tensor([label_token] + list(self.data.iloc[idx].values)[1:] + [self.stop_token], dtype=torch.long)\n",
    "        \n",
    "        inputs = data[:-1]\n",
    "        labels = data[1:]\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab6b1ea-a5b6-489e-bcd4-da98d65b5b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([785])\n",
      "torch.Size([785])\n"
     ]
    }
   ],
   "source": [
    "# taking the dataset for a spin\n",
    "sample_dataset = MNISTDataset(\"train.csv\", )\n",
    "x,y = next(iter(sample_dataset))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a613c4-55f4-4a9a-b526-7d5f0e8c7b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataset into train and validation\n",
    "validation_fraction = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "whole_dataset = MNISTDataset(\"train.csv\")\n",
    "split_idx = int(len(whole_dataset)*validation_fraction)\n",
    "\n",
    "val_indices = [i for i in range(0,split_idx)]\n",
    "train_indices = [i for i in range(split_idx, len(whole_dataset))]\n",
    "\n",
    "val_set = Subset(whole_dataset, val_indices)\n",
    "train_set = Subset(whole_dataset, train_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f77af0-1ab9-410e-9086-f63e83eba8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f639095e-0902-4785-9fc2-27356fb167df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 785]), torch.Size([64, 785]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0cb32-2e80-4f2d-9336-47c7964aa26e",
   "metadata": {},
   "source": [
    "# GPT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1da97900-0f55-4afa-abf7-70ebed313f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class NewGELU(nn.Module):\n",
    "#     def forward(self,x):\n",
    "#         return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e1b5e1b-7f2d-4e2a-95d0-32f83383eb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    block_size: int = 786\n",
    "    vocab_size: int = 320\n",
    "    n_layer: int = 8   # Reduced number of layers\n",
    "    n_head: int = 8    # Reduced number of heads\n",
    "    n_embd: int = 320  # Reduced embedding size\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5f7d84-4095-42a2-bd34-df4adfa652ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6897cfc1-1291-4bd4-a11e-71f7923aafb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # key, query, value projection\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1,1,config.block_size, config.block_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # (B, T, C)\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # (B, T, 3C) --> (B, T, C) (x 3)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, n_h, T, h_dim)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, n_h, T, h_dim)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, n_h, T, h_dim)\n",
    "        \n",
    "        att_scores = (q @ k.transpose(-1, -2)) * (1.0 / math.sqrt(k.size(-1))) # (B, n_h, T, T)\n",
    "        att_scores = att_scores.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf')) \n",
    "        \n",
    "        # print(att_scores.shape)\n",
    "        att_scores = F.softmax(att_scores, dim=-1) # (B, n_h, T, T)\n",
    "        # print(att_scores)\n",
    "        \n",
    "        \n",
    "        att_scores = self.attn_dropout(att_scores)\n",
    "        \n",
    "        out = att_scores @ v # (B, n_h, T, h_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c0ab17-44f1-47fc-9b46-c84590e1a1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6db87a0f-3603-487e-8321-235c58322cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41e749e-0410-4e59-9897-8635787e928d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias)  \n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2* config.n_layer))\n",
    "        print(f\"number of parameters: {self.get_num_params()/1e6:.4f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params = total_params - self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # targets: (B, T)\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # forward the GPT model\n",
    "        tok_emb = self.transformer.wte(idx) # (B,T) --> (B, T, C)\n",
    "        pos_emb = self.transformer.wpe(pos) # (T) --> (1, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "            \n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \n",
    "        while idx.size(1) < self.config.block_size:\n",
    "            \n",
    "            print(idx.size(1))\n",
    "            # print(idx.shape)\n",
    "            logits, _ = self(idx)\n",
    "            # print(logits.shape)\n",
    "            \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "            # print(logits.shape)\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # print(probs.shape)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            \n",
    "        \n",
    "        return idx           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0db349-1fa5-42da-84f1-26a63ae0100f",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2be71207-1923-4e41-9a6e-b764d38b0009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 9.9667\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b441c4-4573-4709-932e-0243055706aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cedeb450-294c-4899-8897-97bc9eacb8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-7\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db6822a5-5878-4853-8dcc-e862ea69a380",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 0.5171\n",
      "Epoch: 0, Iteration: 50, Loss: 0.5608\n",
      "Epoch: 0, Iteration: 100, Loss: 0.5881\n",
      "Epoch: 0, Iteration: 150, Loss: 0.5272\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5006\n",
      "Epoch: 0, Iteration: 250, Loss: 0.5599\n",
      "Epoch: 0, Iteration: 300, Loss: 0.5094\n",
      "Epoch: 0, Iteration: 350, Loss: 0.5248\n",
      "Epoch: 0, Iteration: 400, Loss: 0.5807\n",
      "Epoch: 0, Iteration: 450, Loss: 0.5556\n",
      "Epoch: 0, Iteration: 500, Loss: 0.5109\n",
      "Epoch: 0, Iteration: 550, Loss: 0.5451\n",
      "Epoch 0, Validation Loss: 0.5451\n",
      "Epoch 0 Summary: Train Loss: 0.5435, Validation Loss: 0.5451\n",
      "Epoch: 1, Iteration: 0, Loss: 0.5774\n",
      "Epoch: 1, Iteration: 50, Loss: 0.5555\n",
      "Epoch: 1, Iteration: 100, Loss: 0.4958\n",
      "Epoch: 1, Iteration: 150, Loss: 0.5867\n",
      "Epoch: 1, Iteration: 200, Loss: 0.5675\n",
      "Epoch: 1, Iteration: 250, Loss: 0.5626\n",
      "Epoch: 1, Iteration: 300, Loss: 0.5321\n",
      "Epoch: 1, Iteration: 350, Loss: 0.5206\n",
      "Epoch: 1, Iteration: 400, Loss: 0.5508\n",
      "Epoch: 1, Iteration: 450, Loss: 0.5589\n",
      "Epoch: 1, Iteration: 500, Loss: 0.5766\n",
      "Epoch: 1, Iteration: 550, Loss: 0.5724\n",
      "Epoch 1, Validation Loss: 0.5450\n",
      "Epoch 1 Summary: Train Loss: 0.5434, Validation Loss: 0.5450\n",
      "Epoch: 2, Iteration: 0, Loss: 0.5188\n",
      "Epoch: 2, Iteration: 50, Loss: 0.5717\n",
      "Epoch: 2, Iteration: 100, Loss: 0.5471\n",
      "Epoch: 2, Iteration: 150, Loss: 0.5834\n",
      "Epoch: 2, Iteration: 200, Loss: 0.5261\n",
      "Epoch: 2, Iteration: 250, Loss: 0.5143\n",
      "Epoch: 2, Iteration: 300, Loss: 0.5006\n",
      "Epoch: 2, Iteration: 350, Loss: 0.5551\n",
      "Epoch: 2, Iteration: 400, Loss: 0.5236\n",
      "Epoch: 2, Iteration: 450, Loss: 0.5550\n",
      "Epoch: 2, Iteration: 500, Loss: 0.5431\n",
      "Epoch: 2, Iteration: 550, Loss: 0.5175\n",
      "Epoch 2, Validation Loss: 0.5450\n",
      "Epoch 2 Summary: Train Loss: 0.5433, Validation Loss: 0.5450\n",
      "Epoch: 3, Iteration: 0, Loss: 0.5375\n",
      "Epoch: 3, Iteration: 50, Loss: 0.5119\n",
      "Epoch: 3, Iteration: 100, Loss: 0.6145\n",
      "Epoch: 3, Iteration: 150, Loss: 0.5314\n",
      "Epoch: 3, Iteration: 200, Loss: 0.5321\n",
      "Epoch: 3, Iteration: 250, Loss: 0.5585\n",
      "Epoch: 3, Iteration: 300, Loss: 0.5159\n",
      "Epoch: 3, Iteration: 350, Loss: 0.5863\n",
      "Epoch: 3, Iteration: 400, Loss: 0.5315\n",
      "Epoch: 3, Iteration: 450, Loss: 0.5500\n",
      "Epoch: 3, Iteration: 500, Loss: 0.5438\n",
      "Epoch: 3, Iteration: 550, Loss: 0.5451\n",
      "Epoch 3, Validation Loss: 0.5449\n",
      "Epoch 3 Summary: Train Loss: 0.5432, Validation Loss: 0.5449\n",
      "Epoch: 4, Iteration: 0, Loss: 0.5659\n",
      "Epoch: 4, Iteration: 50, Loss: 0.5528\n",
      "Epoch: 4, Iteration: 100, Loss: 0.5469\n",
      "Epoch: 4, Iteration: 150, Loss: 0.5294\n",
      "Epoch: 4, Iteration: 200, Loss: 0.5280\n",
      "Epoch: 4, Iteration: 250, Loss: 0.5758\n",
      "Epoch: 4, Iteration: 300, Loss: 0.5586\n",
      "Epoch: 4, Iteration: 350, Loss: 0.5383\n",
      "Epoch: 4, Iteration: 400, Loss: 0.5216\n",
      "Epoch: 4, Iteration: 450, Loss: 0.5474\n",
      "Epoch: 4, Iteration: 500, Loss: 0.5424\n",
      "Epoch: 4, Iteration: 550, Loss: 0.5618\n",
      "Epoch 4, Validation Loss: 0.5448\n",
      "Epoch 4 Summary: Train Loss: 0.5432, Validation Loss: 0.5448\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch_num, device, log_every=50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % log_every == 0:\n",
    "            print(f\"Epoch: {epoch_num}, Iteration: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validation_epoch(model, loader, epoch_idx, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    val_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch_idx}, Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, i, device)\n",
    "    val_loss = validation_epoch(model, val_loader, i, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {i} Summary: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c061f70-c324-4264-8ba7-c4302047480c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7c44a59010>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "desired_number = 4\n",
    "idx = torch.tensor([256+desired_number])[None, :].to(device)\n",
    "output = model.generate(idx)\n",
    "plt.imshow(output.cpu()[0, 1:-1].contiguous().view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbff2ec9-a76a-4eb2-9e64-f9a55910b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600a0c7-fc24-4c86-85a6-7d5f0fa3bb48",
   "metadata": {
    "tags": []
   },
   "source": [
    "# evaluation / super cool gif generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16dd9c93-cdc5-439a-b8d3-e994dcc44694",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c105ff-945c-49bb-8acc-62c01c419d42",
   "metadata": {},
   "source": [
    "# load model and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fad206-2cd7-46b2-a15d-b9240f448b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 9.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355848/1944826622.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('trained_model_state_dict.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(320, 320)\n",
       "    (wpe): Embedding(786, 320)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=320, out_features=960, bias=True)\n",
       "          (c_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=320, out_features=320, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "\n",
    "state_dict = torch.load('trained_model_state_dict.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eda73eaa-52b6-4880-a3fa-a5f04fe21c6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82ac684990>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "desired_number = 7\n",
    "idx = torch.tensor([256+desired_number])[None, :].to(device)\n",
    "output = trained_model.generate(idx)\n",
    "plt.imshow(output.cpu()[0, 1:-1].contiguous().view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1684bc-759c-4498-b449-1b9041cb153a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
