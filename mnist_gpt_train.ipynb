{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ca14a5e-c286-4d67-87c6-b8203d65e04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aba42d-a867-4d49-8b1c-34f19910c87b",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17a54337-bf6c-4a5b-ad55-ec615d35cf63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f828850ff50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data.iloc[1].values[1:].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54450440-f39c-4600-973d-0de14bbbf4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.stop_token = 256 + 10 # 256 - 265 correspond to labels, which we can condition on\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label_token = self.data.iloc[idx].values[0] + 256\n",
    "        data = torch.tensor([label_token] + list(self.data.iloc[idx].values)[1:] + [self.stop_token], dtype=torch.long)\n",
    "        \n",
    "        inputs = data[:-1]\n",
    "        labels = data[1:]\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ab6b1ea-a5b6-489e-bcd4-da98d65b5b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([785])\n",
      "torch.Size([785])\n"
     ]
    }
   ],
   "source": [
    "# taking the dataset for a spin\n",
    "sample_dataset = MNISTDataset(\"train.csv\", )\n",
    "x,y = next(iter(sample_dataset))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93a613c4-55f4-4a9a-b526-7d5f0e8c7b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataset into train and validation\n",
    "validation_fraction = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "whole_dataset = MNISTDataset(\"train.csv\")\n",
    "split_idx = int(len(whole_dataset)*validation_fraction)\n",
    "\n",
    "val_indices = [i for i in range(0,split_idx)]\n",
    "train_indices = [i for i in range(split_idx, len(whole_dataset))]\n",
    "\n",
    "val_set = Subset(whole_dataset, val_indices)\n",
    "train_set = Subset(whole_dataset, train_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f77af0-1ab9-410e-9086-f63e83eba8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f639095e-0902-4785-9fc2-27356fb167df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 785]), torch.Size([64, 785]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0cb32-2e80-4f2d-9336-47c7964aa26e",
   "metadata": {},
   "source": [
    "# GPT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da97900-0f55-4afa-abf7-70ebed313f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class NewGELU(nn.Module):\n",
    "#     def forward(self,x):\n",
    "#         return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e1b5e1b-7f2d-4e2a-95d0-32f83383eb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    block_size: int = 786\n",
    "    vocab_size: int = 320\n",
    "    n_layer: int = 8   # Reduced number of layers\n",
    "    n_head: int = 8    # Reduced number of heads\n",
    "    n_embd: int = 320  # Reduced embedding size\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa5f7d84-4095-42a2-bd34-df4adfa652ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6897cfc1-1291-4bd4-a11e-71f7923aafb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # key, query, value projection\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1,1,config.block_size, config.block_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # (B, T, C)\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # (B, T, 3C) --> (B, T, C) (x 3)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, n_h, T, h_dim)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, n_h, T, h_dim)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, n_h, T, h_dim)\n",
    "        \n",
    "        att_scores = (q @ k.transpose(-1, -2)) * (1.0 / math.sqrt(k.size(-1))) # (B, n_h, T, T)\n",
    "        att_scores = att_scores.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf')) \n",
    "        \n",
    "        # print(att_scores.shape)\n",
    "        att_scores = F.softmax(att_scores, dim=-1) # (B, n_h, T, T)\n",
    "        # print(att_scores)\n",
    "        \n",
    "        \n",
    "        att_scores = self.attn_dropout(att_scores)\n",
    "        \n",
    "        out = att_scores @ v # (B, n_h, T, h_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6c0ab17-44f1-47fc-9b46-c84590e1a1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6db87a0f-3603-487e-8321-235c58322cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d41e749e-0410-4e59-9897-8635787e928d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias)  \n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2* config.n_layer))\n",
    "        print(f\"number of parameters: {self.get_num_params()/1e6:.4f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params = total_params - self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # targets: (B, T)\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # forward the GPT model\n",
    "        tok_emb = self.transformer.wte(idx) # (B,T) --> (B, T, C)\n",
    "        pos_emb = self.transformer.wpe(pos) # (T) --> (1, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "            \n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \n",
    "        while idx.size(1) < self.config.block_size:\n",
    "            \n",
    "            print(idx.size(1))\n",
    "            # print(idx.shape)\n",
    "            logits, _ = self(idx)\n",
    "            # print(logits.shape)\n",
    "            \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "            # print(logits.shape)\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # print(probs.shape)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            \n",
    "        \n",
    "        return idx           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0db349-1fa5-42da-84f1-26a63ae0100f",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2be71207-1923-4e41-9a6e-b764d38b0009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 9.9667\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29b441c4-4573-4709-932e-0243055706aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cedeb450-294c-4899-8897-97bc9eacb8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "num_epochs = 8\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db6822a5-5878-4853-8dcc-e862ea69a380",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 5.0777\n",
      "Epoch: 0, Iteration: 50, Loss: 1.1312\n",
      "Epoch: 0, Iteration: 100, Loss: 1.0021\n",
      "Epoch: 0, Iteration: 150, Loss: 1.0153\n",
      "Epoch: 0, Iteration: 200, Loss: 1.0119\n",
      "Epoch: 0, Iteration: 250, Loss: 0.9142\n",
      "Epoch: 0, Iteration: 300, Loss: 0.9569\n",
      "Epoch: 0, Iteration: 350, Loss: 0.9841\n",
      "Epoch: 0, Iteration: 400, Loss: 0.9107\n",
      "Epoch: 0, Iteration: 450, Loss: 0.8585\n",
      "Epoch: 0, Iteration: 500, Loss: 0.8454\n",
      "Epoch: 0, Iteration: 550, Loss: 0.8606\n",
      "Epoch 0, Validation Loss: 0.8430\n",
      "Epoch 0 Summary: Train Loss: 0.9892, Validation Loss: 0.8430\n",
      "Epoch: 1, Iteration: 0, Loss: 0.8722\n",
      "Epoch: 1, Iteration: 50, Loss: 0.8388\n",
      "Epoch: 1, Iteration: 100, Loss: 0.8508\n",
      "Epoch: 1, Iteration: 150, Loss: 0.8298\n",
      "Epoch: 1, Iteration: 200, Loss: 0.8045\n",
      "Epoch: 1, Iteration: 250, Loss: 0.8044\n",
      "Epoch: 1, Iteration: 300, Loss: 0.7258\n",
      "Epoch: 1, Iteration: 350, Loss: 0.7804\n",
      "Epoch: 1, Iteration: 400, Loss: 0.7453\n",
      "Epoch: 1, Iteration: 450, Loss: 0.7321\n",
      "Epoch: 1, Iteration: 500, Loss: 0.7483\n",
      "Epoch: 1, Iteration: 550, Loss: 0.7204\n",
      "Epoch 1, Validation Loss: 0.7221\n",
      "Epoch 1 Summary: Train Loss: 0.7818, Validation Loss: 0.7221\n",
      "Epoch: 2, Iteration: 0, Loss: 0.7287\n",
      "Epoch: 2, Iteration: 50, Loss: 0.7362\n",
      "Epoch: 2, Iteration: 100, Loss: 0.7106\n",
      "Epoch: 2, Iteration: 150, Loss: 0.7046\n",
      "Epoch: 2, Iteration: 200, Loss: 0.6876\n",
      "Epoch: 2, Iteration: 250, Loss: 0.7180\n",
      "Epoch: 2, Iteration: 300, Loss: 0.6262\n",
      "Epoch: 2, Iteration: 350, Loss: 0.7064\n",
      "Epoch: 2, Iteration: 400, Loss: 0.7154\n",
      "Epoch: 2, Iteration: 450, Loss: 0.6754\n",
      "Epoch: 2, Iteration: 500, Loss: 0.6644\n",
      "Epoch: 2, Iteration: 550, Loss: 0.6886\n",
      "Epoch 2, Validation Loss: 0.6518\n",
      "Epoch 2 Summary: Train Loss: 0.6970, Validation Loss: 0.6518\n",
      "Epoch: 3, Iteration: 0, Loss: 0.7038\n",
      "Epoch: 3, Iteration: 50, Loss: 0.6245\n",
      "Epoch: 3, Iteration: 100, Loss: 0.6845\n",
      "Epoch: 3, Iteration: 150, Loss: 0.6743\n",
      "Epoch: 3, Iteration: 200, Loss: 0.6547\n",
      "Epoch: 3, Iteration: 250, Loss: 0.6442\n",
      "Epoch: 3, Iteration: 300, Loss: 0.6491\n",
      "Epoch: 3, Iteration: 350, Loss: 0.6867\n",
      "Epoch: 3, Iteration: 400, Loss: 0.6079\n",
      "Epoch: 3, Iteration: 450, Loss: 0.6745\n",
      "Epoch: 3, Iteration: 500, Loss: 0.6376\n",
      "Epoch: 3, Iteration: 550, Loss: 0.6291\n",
      "Epoch 3, Validation Loss: 0.6170\n",
      "Epoch 3 Summary: Train Loss: 0.6430, Validation Loss: 0.6170\n",
      "Epoch: 4, Iteration: 0, Loss: 0.6274\n",
      "Epoch: 4, Iteration: 50, Loss: 0.6457\n",
      "Epoch: 4, Iteration: 100, Loss: 0.6126\n",
      "Epoch: 4, Iteration: 150, Loss: 0.5925\n",
      "Epoch: 4, Iteration: 200, Loss: 0.6531\n",
      "Epoch: 4, Iteration: 250, Loss: 0.6311\n",
      "Epoch: 4, Iteration: 300, Loss: 0.6349\n",
      "Epoch: 4, Iteration: 350, Loss: 0.5916\n",
      "Epoch: 4, Iteration: 400, Loss: 0.6310\n",
      "Epoch: 4, Iteration: 450, Loss: 0.5803\n",
      "Epoch: 4, Iteration: 500, Loss: 0.6488\n",
      "Epoch: 4, Iteration: 550, Loss: 0.5882\n",
      "Epoch 4, Validation Loss: 0.5943\n",
      "Epoch 4 Summary: Train Loss: 0.6122, Validation Loss: 0.5943\n",
      "Epoch: 5, Iteration: 0, Loss: 0.5969\n",
      "Epoch: 5, Iteration: 50, Loss: 0.6314\n",
      "Epoch: 5, Iteration: 100, Loss: 0.5787\n",
      "Epoch: 5, Iteration: 150, Loss: 0.5588\n",
      "Epoch: 5, Iteration: 200, Loss: 0.6062\n",
      "Epoch: 5, Iteration: 250, Loss: 0.5913\n",
      "Epoch: 5, Iteration: 300, Loss: 0.5991\n",
      "Epoch: 5, Iteration: 350, Loss: 0.5998\n",
      "Epoch: 5, Iteration: 400, Loss: 0.5701\n",
      "Epoch: 5, Iteration: 450, Loss: 0.6350\n",
      "Epoch: 5, Iteration: 500, Loss: 0.5856\n",
      "Epoch: 5, Iteration: 550, Loss: 0.6084\n",
      "Epoch 5, Validation Loss: 0.5776\n",
      "Epoch 5 Summary: Train Loss: 0.5903, Validation Loss: 0.5776\n",
      "Epoch: 6, Iteration: 0, Loss: 0.5506\n",
      "Epoch: 6, Iteration: 50, Loss: 0.5851\n",
      "Epoch: 6, Iteration: 100, Loss: 0.5158\n",
      "Epoch: 6, Iteration: 150, Loss: 0.5740\n",
      "Epoch: 6, Iteration: 200, Loss: 0.5876\n",
      "Epoch: 6, Iteration: 250, Loss: 0.5585\n",
      "Epoch: 6, Iteration: 300, Loss: 0.5781\n",
      "Epoch: 6, Iteration: 350, Loss: 0.5537\n",
      "Epoch: 6, Iteration: 400, Loss: 0.5413\n",
      "Epoch: 6, Iteration: 450, Loss: 0.5774\n",
      "Epoch: 6, Iteration: 500, Loss: 0.6105\n",
      "Epoch: 6, Iteration: 550, Loss: 0.5533\n",
      "Epoch 6, Validation Loss: 0.5652\n",
      "Epoch 6 Summary: Train Loss: 0.5741, Validation Loss: 0.5652\n",
      "Epoch: 7, Iteration: 0, Loss: 0.5716\n",
      "Epoch: 7, Iteration: 50, Loss: 0.5443\n",
      "Epoch: 7, Iteration: 100, Loss: 0.5993\n",
      "Epoch: 7, Iteration: 150, Loss: 0.5482\n",
      "Epoch: 7, Iteration: 200, Loss: 0.5853\n",
      "Epoch: 7, Iteration: 250, Loss: 0.5636\n",
      "Epoch: 7, Iteration: 300, Loss: 0.6119\n",
      "Epoch: 7, Iteration: 350, Loss: 0.5460\n",
      "Epoch: 7, Iteration: 400, Loss: 0.5899\n",
      "Epoch: 7, Iteration: 450, Loss: 0.5284\n",
      "Epoch: 7, Iteration: 500, Loss: 0.5236\n",
      "Epoch: 7, Iteration: 550, Loss: 0.5456\n",
      "Epoch 7, Validation Loss: 0.5541\n",
      "Epoch 7 Summary: Train Loss: 0.5613, Validation Loss: 0.5541\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch_num, device, log_every=50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % log_every == 0:\n",
    "            print(f\"Epoch: {epoch_num}, Iteration: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validation_epoch(model, loader, epoch_idx, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    val_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch_idx}, Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, i, device)\n",
    "    val_loss = validation_epoch(model, val_loader, i, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {i} Summary: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c061f70-c324-4264-8ba7-c4302047480c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7c465752d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "desired_number = 2\n",
    "idx = torch.tensor([256+desired_number])[None, :].to(device)\n",
    "output = model.generate(idx)\n",
    "plt.imshow(output.cpu()[0, 1:-1].contiguous().view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbff2ec9-a76a-4eb2-9e64-f9a55910b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600a0c7-fc24-4c86-85a6-7d5f0fa3bb48",
   "metadata": {
    "tags": []
   },
   "source": [
    "# evaluation / super cool gif generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16dd9c93-cdc5-439a-b8d3-e994dcc44694",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c105ff-945c-49bb-8acc-62c01c419d42",
   "metadata": {},
   "source": [
    "# load model and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8fad206-2cd7-46b2-a15d-b9240f448b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333567/763647393.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('trained_model.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(320, 256)\n",
       "    (wpe): Embedding(786, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=320, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = GPT(GPTConfig())\n",
    "\n",
    "state_dict = torch.load('trained_model.pt')\n",
    "trained_model.load_state_dict(state_dict)\n",
    "trained_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eda73eaa-52b6-4880-a3fa-a5f04fe21c6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7c466fb190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "desired_number = 2\n",
    "idx = torch.tensor([256+desired_number])[None, :].to(device)\n",
    "output = trained_model.generate(idx)\n",
    "plt.imshow(output.cpu()[0, 1:-1].contiguous().view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1684bc-759c-4498-b449-1b9041cb153a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
